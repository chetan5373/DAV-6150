{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caaecd77",
   "metadata": {},
   "source": [
    "### <CENTER><h1><u>Neural Networks</u></CENTER></h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "<CENTER>(TEAM CONTRIBUTORS: BITTERLEIN KONNOTH BIJU, SHASHANK SHEKHAR, CHAITANYA DEVARSHI)</CENTER>\n",
    "\n",
    "-------\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e56e71d",
   "metadata": {},
   "source": [
    "<h2><u>Content</u></h2>\n",
    "\n",
    "[Introduction](#Introduction)\n",
    "\n",
    "- [Problem Statement](#Problem-Statement)\n",
    "- [Methodology](#Methodology)\n",
    "\n",
    "[Data Loading & Manipulation](#Data-Loading-&-Manipulation)\n",
    "\n",
    "[EDA](#Exploratory-Data-Analysis)\n",
    "\n",
    "- [Missing Values](#Identifying-Missing-Values)\n",
    "- [Univariate](#Univariate-Analysis)\n",
    "   - [For numeric features](#Univariate-Analysis-for-numeric-features)\n",
    "   - [For Binary features](#Univariate-Analysis-for-Binary-features)\n",
    "- [Bivariate](#Bivariate)\n",
    "- [Multivariate](#Multivariate)\n",
    "\n",
    "[Data Cleaning](#Data-Cleaning)\n",
    "\n",
    "- [Handling Outliers](#Handling-Outliers)\n",
    "\n",
    "[Feature Selection / Dimensionality Reduction](#Feature-Selection-/-Dimensionality-Reduction)\n",
    "\n",
    "- [Variance Threshold](#Variance-Threshold)\n",
    "\n",
    "- [PCA](#Principal-Component-Analysis)\n",
    "\n",
    "- [Forward Feature Selection](#Forward-Feature-Selection)\n",
    "\n",
    "[Neural Network Modeling](#Neural-Network-Modeling) \n",
    "\n",
    "[Model Selection](#Model-Selection)\n",
    "\n",
    "\n",
    "\n",
    "[Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80172110",
   "metadata": {},
   "source": [
    "------\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4429f2c",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e825481",
   "metadata": {},
   "source": [
    "Mashable is an independent online news site dedicated to covering digital culture, social media and technology. With more than 20 million unique monthly visitors, Mashable has one of the most engaged online news communities.This dataset summarizes a heterogeneous set of features about articles published by Mashable,[Mashable](www.mashable.com) in a period of two years. The goal is to predict the number of shares in social networks (popularity).\n",
    "\n",
    "**Our task for the Module 13 Assignment is to construct and compare / contrast the performance of three separate feed-forward / back propagating neural networks. The response variable we will be modeling will be a categorical indicator variable derived\n",
    "from the dataset’s share attribute.**\n",
    "\n",
    "\n",
    "\n",
    "The data set you will be using is sourced from the UC Irvine machine learning archive:https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity\n",
    "\n",
    "An overview of the data attributes is provided below:\n",
    "\n",
    "__Attribute Information:__\n",
    "\n",
    "|Feature Name | Feature Description      |\n",
    "|:------------| :-----------------------------------------------------------------------------------:| \n",
    "| url                                     | URL of the article (non-predictive)|\n",
    "|timedelta                                |Days between the article publication and the dataset acquisition (non-predictive)\n",
    "|n_tokens_title                           |Number of words in the title\n",
    "|n_tokens_content                         | Number of words in the content\n",
    "|n_unique_tokens                          |Rate of unique words in the content\n",
    "|n_non_stop_words                         |Rate of non-stop words in the content\n",
    "| n_non_stop_unique_tokens                | Rate of unique non-stop words in the content\n",
    "| num_hrefs                               | Number of links\n",
    "| num_self_hrefs                          |Number of links to other articles published by Mashable\n",
    "|num_imgs                                 |Number of images\n",
    "|num_videos                               | Number of videos\n",
    "| average_token_length                    | Average length of the words in the content\n",
    "| num_keywords                            |Number of keywords in the metadata\n",
    "|data_channel_is_lifestyle                | Is data channel 'Lifestyle'?\n",
    "| data_channel_is_entertainment           |Is data channel 'Entertainment'?\n",
    "|data_channel_is_bus                      |Is data channel 'Business'?\n",
    "|data_channel_is_socmed                   |Is data channel 'Social Media'?\n",
    "|data_channel_is_tech                     | Is data channel 'Tech'?\n",
    "|data_channel_is_world                    |Is data channel 'World'?\n",
    "| kw_min_min                              |Worst keyword (min. shares)\n",
    "| kw_max_min                              | Worst keyword (max. shares)\n",
    "| kw_avg_min                              | Worst keyword (avg. shares)\n",
    "| kw_min_max                              |Best keyword (min. shares)\n",
    "| kw_max_max                              |Best keyword (max. shares)\n",
    "| kw_avg_max                              | Best keyword (avg. shares)\n",
    "| kw_min_avg                              |Avg. keyword (min. shares)\n",
    "| kw_max_avg                              |Avg. keyword (max. shares)\n",
    "| kw_avg_avg                              | Avg. keyword (avg. shares)\n",
    "| self_reference_min_shares               |Min. shares of referenced articles in Mashable\n",
    "| self_reference_max_shares               |Max. shares of referenced articles in Mashable\n",
    "|self_reference_avg_sharess               |Avg. shares of referenced articles in Mashable\n",
    "| weekday_is_monday                       |Was the article published on a Monday?\n",
    "| weekday_is_tuesday                      |Was the article published on a Tuesday?\n",
    "|weekday_is_wednesday                     | Was the article published on a Wednesday?\n",
    "|weekday_is_thursday                      |Was the article published on a Thursday?\n",
    "|weekday_is_friday                        |Was the article published on a Friday?\n",
    "|weekday_is_saturday                      |Was the article published on a Saturday?\n",
    "|weekday_is_sunday                        | Was the article published on a Sunday?\n",
    "|is_weekend                               |Was the article published on the weekend?\n",
    "| LDA_00                                  | Closeness to LDA topic 0\n",
    "| LDA_01                                  | Closeness to LDA topic 1\n",
    "| LDA_02                                  | Closeness to LDA topic 2\n",
    "| LDA_03                                  |Closeness to LDA topic 3\n",
    "| LDA_04                                  |Closeness to LDA topic 4\n",
    "| global_subjectivity                     |Text subjectivity\n",
    "|global_sentiment_polarity                |Text sentiment polarity\n",
    "|global_rate_positive_words               | Rate of positive words in the content\n",
    "|global_rate_negative_words               |Rate of negative words in the content\n",
    "| rate_positive_words                     |Rate of positive words among non-neutral tokens\n",
    "| rate_negative_words                     |Rate of negative words among non-neutral tokens\n",
    "|avg_positive_polarity                    |Avg. polarity of positive words\n",
    "| min_positive_polarity                   | Min. polarity of positive words\n",
    "| max_positive_polarity                   |Max. polarity of positive words\n",
    "| avg_negative_polarity                   | Avg. polarity of negative  words\n",
    "| min_negative_polarity                   |Min. polarity of negative  words\n",
    "| max_negative_polarity                   |Max. polarity of negative  words\n",
    "|title_subjectivity                       |Title subjectivity\n",
    "| title_sentiment_polarity                | Title polarity\n",
    "| abs_title_subjectivity                  |Absolute subjectivity level\n",
    "| abs_title_sentiment_polarity            |Absolute polarity level\n",
    "|shares:                                  |Number of shares (target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f69995f",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e14d6e",
   "metadata": {},
   "source": [
    "As we  recall, for that Assignment  we were tasked with apply feature selection and/or dimensionality\n",
    "reduction techniques to identify the explanatory variables to be included within a linear regression model that\n",
    "predicts the number of times an online news article will be shared. Our task for the Module 13 Assignment is\n",
    "to construct and compare / contrast the performance of three separate feed-forward / back propagating\n",
    "neural networks. The response variable We will be modeling will be a categorical indicator variable derived\n",
    "from the dataset’s share attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79fc315",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390c589",
   "metadata": {},
   "source": [
    "<h3><u> To address this project, we will follow these below steps :- </u></h3>\n",
    "\n",
    "1. **Load the dataset**: Upload the ` M4_Data.csv` file from the DAV 6150 Github Repository.\n",
    "\n",
    "2. **Read the dataset**: Using a Jupyter Notebook, read the dataset from the respective Github repository and load it into a Pandas DataFrame.\n",
    "\n",
    "3. **Perform EDA**: Carry out Exploratory Data Analysis to examine the dataset's structure and understand the variables.\n",
    "\n",
    "4. **Identify and rectify issues**: Detect data quality and integrity issues such as missing values or outliers during EDA, and take appropriate actions to address them.\n",
    "\n",
    "5. **Prepped Data Review**: Here, we will cross check every thing and will make sure our data is ready for further analysis.\n",
    "\n",
    "6. **Feature Scale, Selection & Dimensionality Reduction**: Applying feature selection techniques and perform dimensionality reduction to prepare the data for modeling.\n",
    "\n",
    "7. **Neural Network Modeling**: Explain and present our neural network modeling work, including your feature selection / dimensionality reduction decisions and the process by which we selected the hyperparameters for our models.\n",
    " \n",
    "8. **Select Models**: Explain our model selection criteria.Identify our preferred model, then Compare / contrast its performance with that of your other models.  Applying our preferred model to the testing subset and explain our results. \n",
    "\n",
    "9. **Conclusion**: We will conclude our work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c24dfbc",
   "metadata": {},
   "source": [
    "<b> [Back to Content](#Content) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ae6ba",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bb4ada",
   "metadata": {},
   "source": [
    "## Data Loading & Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be539dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing basic Libraries.\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# Importing Libraries for statistical analysis.\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "# Importing Libraries for machine learning models.\n",
    "\n",
    "import sklearn\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "import imblearn\n",
    "from imblearn.metrics import specificity_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, classification_report, confusion_matrix\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "\n",
    "\n",
    "# Importing Libraries for plotting the graphs.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Importing Libraries for Standarising and Normalising.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Import Library for PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Import missingno library for checking on missing values.\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "\n",
    "# Importing train_test_split .\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "\n",
    "\n",
    "# Importing Library for imputing null values.\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "# Importing filterwarnings from warnings to ignore warnings.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c3efd",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data from the github\n",
    "article_df=pd.read_csv(\"https://raw.githubusercontent.com/bitterlein-biju/DAV-6150-/main/M4_Data%20(1).csv\")\n",
    "article_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd37d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifing how many rows and columns the dataframe consist of\n",
    "article_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f0fc6",
   "metadata": {},
   "source": [
    "- The dataset consists of **39644 rows** and **61 columns**, each representing different features about articles published by Mashable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7894e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting a concise summary of the DataFrame \n",
    "article_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d34a51",
   "metadata": {},
   "source": [
    "**Dataset observation:**\n",
    "\n",
    "\n",
    "\n",
    "- Index ranges from 0-39644.\n",
    "\n",
    "- Total number of attributes are 61.\n",
    "\n",
    "- 58 attributes are predictive attributes, 2 are non-predictive, and only one is goal field.\n",
    "\n",
    "\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcae60e",
   "metadata": {},
   "source": [
    "<b> [Back to Content](#Content) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e18f0f",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "- Analyzing a data set for purposes of summarizing its characteristics, identifying relationships between its attributes, and discovering patterns, trends, outliers, missing values and invalid values within the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1193fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking columns names.\n",
    "\n",
    "article_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48916880",
   "metadata": {},
   "source": [
    "- We noticed that some features have extra spaces at the beginning. To resolve this, we'll remove the extra space from the feature names to ensure everything works smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the extra space\n",
    "article_df.columns = article_df.columns.str.strip()\n",
    "\n",
    "article_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c70286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a concise summary of the DataFrame .\n",
    "\n",
    "article_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130620f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropping the 'url' and 'timedelta' columns\n",
    "article_df = article_df.drop(columns=['url', 'timedelta'])\n",
    "article_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a842779a",
   "metadata": {},
   "source": [
    "- As URL and timedelta will not help in analysing the data (Non-predictive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3706e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicate values\n",
    "article_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d521e1",
   "metadata": {},
   "source": [
    "- There are no duplicate values in the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dce13f",
   "metadata": {},
   "source": [
    "### Identifying Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d57043f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking for null values\n",
    "article_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb5b3ea",
   "metadata": {},
   "source": [
    "- There is no missing values. Our dataset for this analysis looks clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the dataset \n",
    "\n",
    "df=article_df.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418bdd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to change the datatype\n",
    "columns_to_convert = [\n",
    "    'data_channel_is_lifestyle', 'data_channel_is_entertainment', 'data_channel_is_bus',\n",
    "    'data_channel_is_socmed', 'data_channel_is_tech', 'data_channel_is_world',\n",
    "    'weekday_is_monday', 'weekday_is_tuesday', 'weekday_is_wednesday', \n",
    "    'weekday_is_thursday', 'weekday_is_friday', 'weekday_is_saturday', \n",
    "    'weekday_is_sunday', 'is_weekend'\n",
    "]\n",
    "\n",
    "# Convert the columns to object data type\n",
    "df[columns_to_convert] = df[columns_to_convert].astype('object')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0608ae04",
   "metadata": {},
   "source": [
    "- Since these features are categorical, changing  their datatype to object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daff65d",
   "metadata": {},
   "source": [
    "<b> [Back to Content](#Content) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a090a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d28b0a",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3780b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to plot dist and box plot for all the numeric features. \n",
    "\n",
    "def box_dist_plot(df , column):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    This function is to plot box-plot and distribution-plot for a given column, \n",
    "    column's median value, with count and percentage of null values. \n",
    "    \n",
    "    Parameters :-\n",
    "        df : Dataframe           # df contains Dataframe.\n",
    "        column : str             # Column name which is to be ploted.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.style.use('ggplot')  \n",
    "    \n",
    "    plt.figure(figsize=(18, 7))\n",
    "\n",
    "    # Box plot.\n",
    "    plt.subplot(121)\n",
    "    sns.boxplot(y = df[column])  # Create box plot\n",
    "    plt.title(f'Box Plot of : {column}')\n",
    "\n",
    "    # Distribution plot.\n",
    "    plt.subplot(122)\n",
    "    sns.histplot(df[column], bins=30, kde=True)  # Create histogram with KDE\n",
    "    plt.title(f'Distribution Plot of : {column}')\n",
    "\n",
    "    # Adjusting the layout.\n",
    "    plt.tight_layout() \n",
    "\n",
    "    plt.show()  \n",
    "\n",
    "    # To print statistics.\n",
    "    print(df[column].describe())\n",
    "    print('Median :', df[column].median())\n",
    "    print()\n",
    "    print('Total Number of null values :', df[column].isnull().sum(), 'count,', \n",
    "          round(df[column].isnull().mean() * 100, 2), '%')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c7b35",
   "metadata": {},
   "source": [
    "### For Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the function \n",
    "box_dist_plot(df,'n_tokens_title')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e93297",
   "metadata": {},
   "source": [
    "- In here n_tokens_title represents Number of words in the title The data is normally distributed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e1304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function\n",
    "box_dist_plot(df, 'n_tokens_content')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df17d73",
   "metadata": {},
   "source": [
    "- This, n_tokens_content represents the number of words in the content. The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec96b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "box_dist_plot(df, 'n_unique_tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bf1108",
   "metadata": {},
   "source": [
    "- This,n_unique_tokens represents the Rate of unique words in the content. The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ec83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function\n",
    "box_dist_plot(df, 'n_non_stop_words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbb7f23",
   "metadata": {},
   "source": [
    "- This, n_non_stop_words represents the Rate of non-stop words in the content . The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "box_dist_plot(df, 'n_non_stop_unique_tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5821de6",
   "metadata": {},
   "source": [
    "- This,n_non_stop_unique_tokens  represents the Rate of unique non-stop words in the content . The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b845bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the function\n",
    "box_dist_plot(df, 'num_hrefs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79566739",
   "metadata": {},
   "source": [
    "- This,num_hrefs represents the Number of links . The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864f8b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the function \n",
    "box_dist_plot(df, 'num_self_hrefs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573275cb",
   "metadata": {},
   "source": [
    "- This,num_self_hrefs represents the Number of links to other articles published by Mashable . The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d415364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function\n",
    "box_dist_plot(df, 'num_imgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05dbc8",
   "metadata": {},
   "source": [
    "- This,num_imgs represents the Number of images. The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284dc82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function\n",
    "box_dist_plot(df, 'num_videos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cde3bd9",
   "metadata": {},
   "source": [
    "- This num_videos,represents the Number of videos The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a62d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function\n",
    "box_dist_plot(df, 'average_token_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cf7e36",
   "metadata": {},
   "source": [
    "- This average_token_length ,represents the Average length of the words in the content. The data is normally distributed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44006c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function\n",
    "box_dist_plot(df, 'num_keywords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a08504",
   "metadata": {},
   "source": [
    "- This num_keywords,represents the Number of keywords in the metadata The data is left-skewed and contains only one outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the function\n",
    "box_dist_plot(df, 'kw_min_min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545e27c5",
   "metadata": {},
   "source": [
    "- The kw_min_min represents Worst keyword minimum share.The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf9e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the function\n",
    "box_dist_plot(df, 'kw_max_min')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dff7073",
   "metadata": {},
   "source": [
    "-  The kw_max_min represents the worst keyword's maximum share. The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a200d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the function\n",
    "box_dist_plot(df, 'kw_avg_min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b099e671",
   "metadata": {},
   "source": [
    "-  The kw_avg_min represents the worst keyword's average share. The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d779fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'kw_min_max')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99aaf4d",
   "metadata": {},
   "source": [
    "- The kw_min_max represents the best keyword's minimum share. The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b2d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'kw_max_max')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7912067d",
   "metadata": {},
   "source": [
    "- The kw_max_max represents the best keyword's maximum share. The data is left-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157db675",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'kw_avg_max')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd80d7c6",
   "metadata": {},
   "source": [
    "- The kw_avg_max represents the best keyword's average share. The data is right-skewed and contains outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b16a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'kw_min_avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c090e31",
   "metadata": {},
   "source": [
    "- The kw_min_avg represents the average keyword's minimum share. The data is right-skewed and contains no  outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'kw_max_avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f88e349",
   "metadata": {},
   "source": [
    "- The kw_max_avg represents the average keyword's maximum share. The data is right-skewed and contains outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5803d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'kw_avg_avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772899d6",
   "metadata": {},
   "source": [
    "- The kw_avg_avg represents the average keyword's average share. The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a5b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'self_reference_min_shares')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98826fc2",
   "metadata": {},
   "source": [
    "- The self_reference_min_shares represents the minimum shares of referenced articles in Mashable. The data is right-skewed and contains outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d41ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'self_reference_max_shares')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb42d10d",
   "metadata": {},
   "source": [
    "- The self_reference_max_shares represents the maximum shares of referenced articles in Mashable. The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eba0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'self_reference_avg_sharess')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458de729",
   "metadata": {},
   "source": [
    "-  The self_reference_avg_shares represents the average shares of referenced articles in Mashable. The data is right-skewed and contains outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c52fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'LDA_00')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a398d54",
   "metadata": {},
   "source": [
    "- The LDA_00 represents closeness to LDA topic 0. The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a9c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'LDA_01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcc1b2e",
   "metadata": {},
   "source": [
    "- The LDA_01 represents closeness to LDA topic 1. The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e10f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'LDA_02')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf1490",
   "metadata": {},
   "source": [
    "- The LDA_02 represents closeness to LDA topic 2. The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951f4505",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'LDA_03')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae0089d",
   "metadata": {},
   "source": [
    "- The LDA_03 represents closeness to LDA topic 3. The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac529ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'LDA_04')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57542b39",
   "metadata": {},
   "source": [
    "-  The LDA_04 represents closeness to LDA topic 4. The data is right-skewed and contains no outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c1ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "box_dist_plot(df, 'global_subjectivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b599966b",
   "metadata": {},
   "source": [
    "- The global_subjectivity represents the text's subjectivity. The data is normally distributed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "box_dist_plot(df, 'global_sentiment_polarity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051f98f8",
   "metadata": {},
   "source": [
    "- The global_sentiment_polarity represents the text's sentiment polarity. The data is normally distributed  and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b389603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function the calling \n",
    "box_dist_plot(df, 'global_rate_positive_words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c12b2b",
   "metadata": {},
   "source": [
    "- The global_rate_positive_words represents the rate of positive words in the content. The data is right-skewed and contains outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "box_dist_plot(df, 'global_rate_negative_words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef82854",
   "metadata": {},
   "source": [
    "- The global_rate_negative_words represents the rate of negative words in the content. The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7bef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "\n",
    "box_dist_plot(df, 'rate_positive_words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b516c",
   "metadata": {},
   "source": [
    "- The rate_positive_words represents the rate of positive words among non-neutral tokens. The data is left-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a582d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "box_dist_plot(df, 'rate_negative_words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de9664a",
   "metadata": {},
   "source": [
    "- The rate_negative_words represents the rate of negative words among non-neutral tokens. The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978885c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "box_dist_plot(df, 'avg_positive_polarity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec901dcb",
   "metadata": {},
   "source": [
    "- The avg_positive_polarity represents the average polarity of positive words. The data is normally distributed  and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c76174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "box_dist_plot(df, 'min_positive_polarity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0835dc",
   "metadata": {},
   "source": [
    "- The min_positive_polarity represents the minimum polarity of positive words. The data is right-skewed and contains outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6086861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "box_dist_plot(df, 'max_positive_polarity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72efe46f",
   "metadata": {},
   "source": [
    "- The max_positive_polarity represents the maximum polarity of positive words. The data is left-skewed and contains no outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8500d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "box_dist_plot(df, 'avg_negative_polarity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65996ff",
   "metadata": {},
   "source": [
    "- The avg_negative_polarity represents the average polarity of negative words. The data is left-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f125865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "\n",
    "box_dist_plot(df, 'min_negative_polarity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dd781d",
   "metadata": {},
   "source": [
    "- The min_negative_polarity represents the minimum polarity of negative words. The data is normally distributed and contains no  outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "box_dist_plot(df, 'max_negative_polarity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d37babe",
   "metadata": {},
   "source": [
    "- The max_negative_polarity represents the maximum polarity of negative words. The data is left-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25866e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "\n",
    "box_dist_plot(df, 'title_subjectivity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4d3627",
   "metadata": {},
   "source": [
    "- The title_subjectivity represents the subjectivity level. The data is right-skewed and contains no  outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e971221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "box_dist_plot(df, 'title_sentiment_polarity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711f46e",
   "metadata": {},
   "source": [
    "- The title_sentiment_polarity measures the sentiment polarity. The data is normally distributed  and contains outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f74b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "box_dist_plot(df, 'abs_title_subjectivity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df40eed",
   "metadata": {},
   "source": [
    "- The abs_title_subjectivity represents the absolute subjectivity level . The data is left-skewed and contains no outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a3b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "box_dist_plot(df, 'abs_title_sentiment_polarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04399b6c",
   "metadata": {},
   "source": [
    "- The abs_title_sentiment_polarity represents the absolute polarity level . The data is right-skewed and contains outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8299979",
   "metadata": {},
   "source": [
    "### For Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a119c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a count plot and displays the count of each category for a specified column in the dataframe.\n",
    "\n",
    "def plot_category_counts(df, column):\n",
    "    \n",
    "    \"\"\"\n",
    "    A function to plot a countplot and  displays the count of each category \n",
    "    for a specified column in the dataframe.\n",
    "    \n",
    "        column : str\n",
    "        The name of the categorical column to plot and count.\n",
    "    \"\"\"\n",
    "\n",
    "    # Count plot for the specified column.\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, x=column, palette=\"viridis\")\n",
    "    \n",
    "\n",
    "    # Set plot labels and title\n",
    "    plt.xlabel(column)  \n",
    "    plt.ylabel('Count')     \n",
    "    plt.xticks(rotation=90, ha='right')\n",
    "    plt.title(f'Count of {column}')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Display count of each category\n",
    "    counts = df[column].value_counts()\n",
    "    print(f\"\\nCounts for {column}:\\n{counts}\")\n",
    "\n",
    "    #For unique count of input\n",
    "    unique_count = df[column].nunique()\n",
    "    print(f\"\\nUnique for {column}:\\n{unique_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637dff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_counts(df, 'data_channel_is_lifestyle')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d86453",
   "metadata": {},
   "source": [
    " - The data_channel_is_lifestyle represents whether the data channel is 'Lifestyle'.Contains 2 unique values zero and one, in this count of zero is the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6031224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_counts(df, 'data_channel_is_entertainment')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3231e96a",
   "metadata": {},
   "source": [
    "- The data_channel_is_entertainment represents whether the data channel is 'Entertainment'. It contains two unique values 0 and 1, with the count of 0 being the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ae9945",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_counts(df, 'data_channel_is_bus')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe6c0f1",
   "metadata": {},
   "source": [
    "- The data_channel_is_bus represents whether the data channel is 'Business'. It contains two unique values 0 and 1, with the count of 0 being the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f07d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_counts(df, 'data_channel_is_socmed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8bbeeb",
   "metadata": {},
   "source": [
    "- The data_channel_is_socmed represents whether the data channel is 'Social Media'. It contains two unique values 0 and 1, with the count of 0 being the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e46b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_counts(df, 'data_channel_is_tech')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c325cc",
   "metadata": {},
   "source": [
    "- The data_channel_is_tech represents whether the data channel is 'Tech'. It contains two unique values 0 and 1, with the count of 0 being the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_counts(df, 'data_channel_is_world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e385c",
   "metadata": {},
   "source": [
    "- The data_channel_is_world represents whether the data channel is 'World'. It contains two unique values 0 and 1, with the count of 0 being the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_counts(df, 'weekday_is_monday')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff66e9",
   "metadata": {},
   "source": [
    "- The weekday_is_monday represents whether the article was published on a Monday. It contains two unique values 0 and 1, with the count of 0 being the highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5450a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_counts(df, 'weekday_is_tuesday')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff75a1d0",
   "metadata": {},
   "source": [
    "- The weekday_is_tuesday represents whether the article was published on a Tuesday. It contains two unique values 0 and 1, with the count of 0 being the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1012a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_counts(df, 'weekday_is_wednesday')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4387c2cf",
   "metadata": {},
   "source": [
    "- The weekday_is_wednesday represents whether the article was published on a Wednesday. It contains two unique values 0 and 1, with the count of 0 being the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f66ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_counts(df, 'weekday_is_thursday')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771ec6c0",
   "metadata": {},
   "source": [
    "- The weekday_is_thursday represents whether the article was published on a Thursday. It contains two unique values 0 and 1, with the count of 0 being the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3047c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_counts(df, 'weekday_is_friday')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6880879f",
   "metadata": {},
   "source": [
    "- The weekday_is_friday represents whether the article was published on a Friday. It contains two unique values 0 and 1, with the count of 0 being the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0adb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_counts(df, 'weekday_is_saturday')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453cf81c",
   "metadata": {},
   "source": [
    "- The weekday_is_saturday represents whether the article was published on a Saturday. It contains two unique values 0 and 1, with the count of 0 being the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ef8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_counts(df, 'weekday_is_sunday')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb4573",
   "metadata": {},
   "source": [
    "- The weekday_is_sunday represents whether the article was published on a Sunday. It contains two unique values 0 and 1, with the count of 0 being the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3165c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_counts(df, 'is_weekend')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a424fd",
   "metadata": {},
   "source": [
    "- The is_weekend represents whether the article was published on the weekend (Saturday or Sunday). It contains two unique values 0 and 1, with the count of 0 being the highest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a4c317",
   "metadata": {},
   "source": [
    "<b> [Back to Content](#Content) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d5497d",
   "metadata": {},
   "source": [
    "<b> [Back to Content](#Content) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ba79ab",
   "metadata": {},
   "source": [
    "## Bivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de59e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find Correlation between the numerical columns to do bivariate analysis.\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6824e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatterplot  to find the insight depends on n_unique_tokens and n_non_stop_words Column.\n",
    "sns.scatterplot(x='n_unique_tokens', y='n_non_stop_words' ,data=df, color='c') \n",
    "plt.title('Scatter Plot:Relation between n_unique_tokens and n_non_stop_words ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd928c8",
   "metadata": {},
   "source": [
    "- It has strong Positive correlation of .99.\n",
    "- When the number of n_unique_tokens increases then n_non_stop_words will aslo increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b661ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatterplot  to find the insight depends on n_unique_tokens and n_non_stop_unique_tokens Column.\n",
    "\n",
    "sns.scatterplot(x='n_unique_tokens', y='n_non_stop_unique_tokens' ,data=df, color='c')  \n",
    "plt.title('Scatter Plot:Relation between n_unique_tokens and n_non_stop_unique_tokens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee2bbc0",
   "metadata": {},
   "source": [
    "- It has strong Positive correlation of .99.\n",
    "- When the number of n_unique_tokens increases then n_non_stop_unique_tokens will aslo increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatterplot  to find the insight depends on n_non_stop_words and n_non_stop_unique_tokens Column.\n",
    "\n",
    "sns.scatterplot(x='n_non_stop_words', y='n_non_stop_unique_tokens' ,data=df, color='c')  \n",
    "plt.title('Scatter Plot:Relation between n_non_stop_words and n_non_stop_unique_tokens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389ae72",
   "metadata": {},
   "source": [
    "- It has strong Positive correlation of .99.\n",
    "- When the number of n_non_stop_words increases then n_non_stop_unique_tokens will aslo increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatterplot  to find the insight depends on avg_negative_polarity and min_negative_polarity Column.\n",
    "\n",
    "sns.scatterplot(x='avg_negative_polarity', y='min_negative_polarity' ,data=df, color='c')  \n",
    "plt.title('Scatter Plot:Relation between avg_negative_polarity and min_negative_polarity ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c28496",
   "metadata": {},
   "source": [
    "- It has positive correlation of .74\n",
    "- When the number of avg_negative_polarity increases then min_negative_polarity will aslo increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e133b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatterplot  to find the insight depends on abs_title_sentiment_polarity and title_sentiment_polarity Column.\n",
    "\n",
    "sns.scatterplot(x='abs_title_sentiment_polarity', y='title_sentiment_polarity' ,data=df, color='c')  \n",
    "plt.title('Scatter Plot:Relation between abs_title_sentiment_polarity and title_sentiment_polarity ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c1a77",
   "metadata": {},
   "source": [
    "- There is no clear linear relation between the two columns.\n",
    "- They are mirror image . So we can not tell, which is increasing and which is decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3915a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatterplot  to find the insight depends on avg_positive_polarity and max_negative_polarity Column.\n",
    "\n",
    "sns.scatterplot(x='avg_positive_polarity', y='max_negative_polarity' ,data=df, color='c')  \n",
    "plt.title('Scatter Plot:Relation between avg_positive_polarity and  max_negative_polarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3253cb",
   "metadata": {},
   "source": [
    "- It has positive correlation of .70\n",
    "- When the number of avg_negative_polarity increases then min_negative_polarity will aslo increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bfd56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatterplot  to find the insight depends on title_subjectivity and abs_title_sentiment_polarity Column.\n",
    "\n",
    "sns.scatterplot(x='title_subjectivity', y='abs_title_sentiment_polarity' ,data=df, color='c') \n",
    "plt.title('Scatter Plot:Relation between title_subjectivity and abs_title_sentiment_polarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a4338",
   "metadata": {},
   "source": [
    "- It has positive correlation of .71\n",
    "- When the number of avg_negative_polarity increases then min_negative_polarity will aslo increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b908862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatterplot  to find the insight depends on title_sentiment_polarity and abs_title_sentiment_polarity Column.\n",
    "\n",
    "sns.scatterplot(x='title_sentiment_polarity', y='abs_title_sentiment_polarity' ,data=df, color='c')  \n",
    "plt.title('Scatter Plot: Relation between title_sentiment_polarity and abs_title_sentiment_polarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbc9c96",
   "metadata": {},
   "source": [
    "- There is no clear linear relation between the two columns.\n",
    "- They are mirror image . So we can not tell, which is increasing and which is decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75a9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting bargraph  to find the insight depends on data_channel_is_lifestyle and global_subjectivity Column.\n",
    "\n",
    "sns.barplot(x='data_channel_is_lifestyle', y='global_subjectivity' ,data=df, color='c')  \n",
    "plt.title('Bar Plot: Cases when data_channel_is_lifestyle')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2af43c",
   "metadata": {},
   "source": [
    "-  Text Subjectivity is good when the binary value of data_channel_is_lifestyle is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd9e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting bargraph  to find the insight depends on data_channel_is_entertainment and global_sentiment_polarity Column.\n",
    "\n",
    "sns.barplot(x='data_channel_is_entertainment', y='global_sentiment_polarity' ,data=df, color='c')  \n",
    "plt.title('Bar Plot:Cases when data_channel_is_entertainment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d56d5a",
   "metadata": {},
   "source": [
    "- Text sentiment polarity is not good when the binary value of data_channel_is_entertainment is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb14e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting bargraph  to find the insight depends on data_channel_is_tech and global_rate_negative_words Column.\n",
    "\n",
    "sns.barplot(x='data_channel_is_tech', y='global_rate_negative_words' ,data=df, color='c')  \n",
    "plt.title('Bar Plot:Cases when data_channel_is_tech')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5677a8",
   "metadata": {},
   "source": [
    "- Rate of negative words in the content is less when the binary value of data_channel_is_tech is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0703074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting bargraph  to find the insight depends on data_channel_is_world and title_subjectivity Column.\n",
    "\n",
    "sns.barplot(x='data_channel_is_world', y='title_subjectivity' ,data=df, color='c')  \n",
    "plt.title('Bar Plot:Cases when data_channel_is_world')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc95f21f",
   "metadata": {},
   "source": [
    "- Title_subjectivity in the content is less when the binary value of data_channel_is_world is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ce5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting bargraph  to find the insight depends on weekday_is_wednesday and self_reference_max_shares Column.\n",
    "\n",
    "sns.barplot(x='weekday_is_wednesday', y='self_reference_max_shares' ,data=df, color='c') \n",
    "plt.title('Bar Plot:Cases when weekday_is_wednesday')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3edea",
   "metadata": {},
   "source": [
    "-  Max. shares of referenced articles in Mashabl is same on wednesday or on another day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039ee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting bargraph  to find the insight depends on weekday_is_sunday and self_reference_avg_sharess Column.\n",
    "\n",
    "sns.barplot(x='weekday_is_sunday', y='self_reference_avg_sharess' ,data=df, color='c') \n",
    "plt.title('Bar Plot:Cases when weekday_is_sunday ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff3f06a",
   "metadata": {},
   "source": [
    "- Avg. shares of referenced articles in Mashable on sunday is less compared to other day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286fece",
   "metadata": {},
   "source": [
    "<b> [Back to Content](#Content) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce46de6",
   "metadata": {},
   "source": [
    "## Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fac718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of dataframe for multivariate analysis and make a new column where all week days will be in \n",
    "# one column, \n",
    "# 1 = weekday_is_monday\n",
    "# 2 = weekday_is_tuesday\n",
    "# 3 = weekday_is_wednesday\n",
    "# 4 = weekday_is_thursday\n",
    "# 5 = weekday_is_friday\n",
    "# 6 = weekday_is_saturday\n",
    "# 7 = weekday_is_sunday\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['day_of_week'] = df.apply(lambda row: (1 if row['weekday_is_monday'] == 1 else \n",
    "                                          2 if row['weekday_is_tuesday'] == 1 else \n",
    "                                          3 if row['weekday_is_wednesday'] == 1 else\n",
    "                                          4 if row['weekday_is_thursday'] == 1 else\n",
    "                                          5 if row['weekday_is_friday'] == 1 else\n",
    "                                          6 if row['weekday_is_saturday'] == 1 else\n",
    "                                          7 if row['weekday_is_sunday'] == 1 else None), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c572ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the unique values in day_of_week.\n",
    "df.day_of_week.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b05b58",
   "metadata": {},
   "source": [
    "- As the published article will be on unique days, so there will not be any collapse of data in the new column but still will cross verify it in the below code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7566ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To cross-verify the numbers of each week days from their respective columns.\n",
    "\n",
    "a = article_df.weekday_is_monday[article_df.weekday_is_monday == 1].value_counts().iloc[0]\n",
    "b = df[df.day_of_week == 1].day_of_week.value_counts().iloc[0]\n",
    "print('Checked for Monday - ', a==b)\n",
    "\n",
    "a = article_df.weekday_is_tuesday[article_df.weekday_is_tuesday == 1].value_counts().iloc[0]\n",
    "b = df[df.day_of_week == 2].day_of_week.value_counts().iloc[0]\n",
    "print('Checked for Tuesday - ', a==b)\n",
    "\n",
    "a = article_df.weekday_is_wednesday[article_df.weekday_is_wednesday == 1].value_counts().iloc[0]\n",
    "b = df[df.day_of_week == 3].day_of_week.value_counts().iloc[0]\n",
    "print('Checked for Wednesday - ', a==b)\n",
    "\n",
    "a = article_df.weekday_is_thursday[article_df.weekday_is_thursday == 1].value_counts().iloc[0]\n",
    "b = df[df.day_of_week == 4].day_of_week.value_counts().iloc[0]\n",
    "print('Checked for Thursday - ', a==b)\n",
    "\n",
    "a = article_df.weekday_is_friday[article_df.weekday_is_friday == 1].value_counts().iloc[0]\n",
    "b = df[df.day_of_week == 5].day_of_week.value_counts().iloc[0]\n",
    "print('Checked for Friday - ', a==b)\n",
    "\n",
    "a = article_df.weekday_is_saturday[article_df.weekday_is_saturday == 1].value_counts().iloc[0]\n",
    "b = df[df.day_of_week == 6].day_of_week.value_counts().iloc[0]\n",
    "print('Checked for Saturday - ', a==b)\n",
    "\n",
    "a = article_df.weekday_is_sunday[article_df.weekday_is_sunday == 1].value_counts().iloc[0]\n",
    "b = df[df.day_of_week == 7].day_of_week.value_counts().iloc[0]\n",
    "print('Checked for Sunday - ', a==b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8e9ce4",
   "metadata": {},
   "source": [
    "- So, we have verified that there isn't any collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f91fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for understanding about the new column, plot the barplot for day of week with respect to shares.\n",
    "\n",
    "df.groupby('day_of_week')[['shares']].count().plot.bar(color='#beb9db')\n",
    "plt.xticks(rotation=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4a0b5",
   "metadata": {},
   "source": [
    "- As per the graph, we can conclude that:- \n",
    "    - Wednesday, followed by tuesday, and then thursday has the maximum counts of shares\n",
    "    - monday, & then friday are with the moderate counts\n",
    "    - weekends has the least counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038887b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new column where all types of channel will be in one column,\n",
    "# Lifestyle = data_channel_is_lifestyle\n",
    "# Entertainment = data_channel_is_entertainment\n",
    "# Bus = data_channel_is_bus\n",
    "# SocMed = data_channel_is_socmed\n",
    "# Tech = data_channel_is_tech\n",
    "# World = data_channel_is_world\n",
    "# Others = if the data where all channels type is zero(0), then 'Others'. \n",
    "\n",
    "df['data_channels'] = df.apply(lambda row: ('Lifestyle' if row['data_channel_is_lifestyle'] == 1 else \n",
    "                                          'Entertainment' if row['data_channel_is_entertainment'] == 1 else \n",
    "                                          'Bus' if row['data_channel_is_bus'] == 1 else\n",
    "                                          'SocMed' if row['data_channel_is_socmed'] == 1 else\n",
    "                                          'Tech' if row['data_channel_is_tech'] == 1 else\n",
    "                                          'World' if row['data_channel_is_world'] == 1 else 'Others'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361c4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the unique values in data_channels.\n",
    "df.data_channels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a5a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the data is null or not.\n",
    "\n",
    "df.data_channels.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab82f5",
   "metadata": {},
   "source": [
    "- No Nulls are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab0d378-38e8-43a5-8d99-329ed2724f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for understanding about the new column, will plot a barplot.\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "df.data_channels.value_counts().plot.bar(color='brown')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e6b7e-d798-4e0f-b611-e7f442502357",
   "metadata": {},
   "source": [
    "- Data channel 'World' has the maximum count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d527dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the color code from heatmap we can clearly states which has the max and min counts of shares with respect to\n",
    "# day_of_week, and data_channels\n",
    "\n",
    "# Setting the style\n",
    "plt.style.use('default')\n",
    "\n",
    "# Group the data and aggregate it\n",
    "grouped_data = df.groupby(['day_of_week', 'data_channels'])['shares'].count().reset_index()\n",
    "\n",
    "# Plotting the barplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=grouped_data,\n",
    "    x='day_of_week', \n",
    "    y='shares', \n",
    "    hue='data_channels', \n",
    "    palette='crest'\n",
    ")\n",
    "\n",
    "# Adding labels and a title\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Count of Shares')\n",
    "plt.title('Barplot of Shares by Day of the Week and Data Channels')\n",
    "plt.legend(title='Data Channels')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9822c1",
   "metadata": {},
   "source": [
    "- Data Channel 'World' has the maximum count of shares on Tuesday , Wednesday , Thrusday , Friday and Sunday.\n",
    "- On Monday Entertainment channel has the maximum count of shares.\n",
    "- On Saturday Tech channel has the maximum count of shares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2870b48a",
   "metadata": {},
   "source": [
    "<b> [Back to Content](#Content) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef736e",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5726b0a9",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2013a5c2",
   "metadata": {},
   "source": [
    "#### Prepare Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb0e96d-2c3b-49bd-85e0-e6f807902172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median percentage of shares\n",
    "median_shares = df[\"shares\"].median()\n",
    "\n",
    "# Create a new categorical variable based on the conditions using lambda function\n",
    "df['shares_level'] = df['shares'].apply(lambda x: 'low' if x < 0.5 * median_shares \n",
    "                                        else ('medium' if 0.5 * median_shares < x <= 1.5 * median_shares \n",
    "                                              else ('high' if x > 1.5 * median_shares else np.nan)))\n",
    "\n",
    "df['shares_level'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ecfecb-5220-486e-9f96-8054e129e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing  the \"shares\"  attributes\n",
    "df.drop(['shares'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc876c-e0e9-4117-8c1a-4c3a5e7b6991",
   "metadata": {},
   "source": [
    "- This is done to eliminate result from the addition of the “share_level” indicator to our collection of attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c38ebbb",
   "metadata": {},
   "source": [
    "###   Handling Skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a30872c",
   "metadata": {},
   "source": [
    "- We observed that the majority of the numerical columns have skewness. To address this issue, we are implementing the Box-Cox method. Before proceeding checking if there are any zero values or negative values in each feature since Box-Cox works better with positive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24065219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for negative value.\n",
    "# Selecting only numeric columns \n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "negative_check = df[numeric_cols].apply(lambda x: (x < 0).any())\n",
    "\n",
    "print(negative_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe6778",
   "metadata": {},
   "source": [
    "- we have negative values in most of the attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4437a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for zero values.\n",
    "\n",
    "zero_check = df[numeric_cols].apply(lambda x: (x == 0).any())\n",
    "\n",
    "print(zero_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b85e8c",
   "metadata": {},
   "source": [
    "- Zero values are present. To handle both zero and negative value, we shift the distribution via the addition of a single data value to all data values within the variable, e.g., if the smallest value in the distribution is -8 we would add 9 (since 8+1 will eliminate the largest negative value) to every data value within the variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ee0f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "columns = [\n",
    "    'n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
    "    'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'num_imgs',\n",
    "    'num_videos', 'average_token_length', 'num_keywords', 'kw_min_min', 'kw_max_min',\n",
    "    'kw_avg_min', 'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg',\n",
    "    'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares',\n",
    "    'self_reference_avg_sharess', 'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04',\n",
    "    'global_subjectivity', 'global_sentiment_polarity', 'global_rate_positive_words',\n",
    "    'global_rate_negative_words', 'rate_positive_words', 'rate_negative_words',\n",
    "    'avg_positive_polarity', 'min_positive_polarity', 'max_positive_polarity',\n",
    "    'avg_negative_polarity', 'min_negative_polarity', 'max_negative_polarity',\n",
    "    'title_subjectivity', 'title_sentiment_polarity', 'abs_title_subjectivity',\n",
    "    'abs_title_sentiment_polarity'\n",
    "]\n",
    "\n",
    "for col in columns:\n",
    "    #  Calculating  the minimum value of the column\n",
    "    min_val = df[col].min()\n",
    "    \n",
    "    #  Determining the shift anount \n",
    "    shift = abs(min_val) + 1 if min_val <= 0 else 0\n",
    "    \n",
    "#    applying  the shift\n",
    "    df[col] += shift\n",
    "    \n",
    "    # Perform Box-Cox transformation and save the lambda value\n",
    "    fitted_data, fitted_lambda = stats.boxcox(df[col])\n",
    "    \n",
    "    #  Replace the original column with transformed data\n",
    "    df[col] = fitted_data\n",
    "    \n",
    "    #  Print the lambda value for each column\n",
    "    print(f\"Lambda value for {col}: {fitted_lambda}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea9b14b",
   "metadata": {},
   "source": [
    " ## 4.2 Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488fe359",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_colms = [\n",
    "    'n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
    "    'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'num_imgs',\n",
    "    'num_videos', 'average_token_length', 'num_keywords', 'kw_min_min', 'kw_max_min',\n",
    "    'kw_avg_min', 'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg',\n",
    "    'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares',\n",
    "    'self_reference_avg_sharess', 'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04',\n",
    "    'global_subjectivity', 'global_sentiment_polarity', 'global_rate_positive_words',\n",
    "    'global_rate_negative_words', 'rate_positive_words', 'rate_negative_words',\n",
    "    'avg_positive_polarity', 'min_positive_polarity', 'max_positive_polarity',\n",
    "    'avg_negative_polarity', 'min_negative_polarity', 'max_negative_polarity',\n",
    "    'title_subjectivity', 'title_sentiment_polarity', 'abs_title_subjectivity',\n",
    "    'abs_title_sentiment_polarity'\n",
    "]\n",
    "# Looping through numeric columns to get the lower and upper bound values.  \n",
    "for col in numeric_colms:\n",
    "    q1 = np.quantile(df[col], 0.25)\n",
    "    q3 = np.quantile(df[col], 0.75)\n",
    "    iqr = q3 - q1\n",
    "    upper_bound = q3 + (1.5 * iqr)                 \n",
    "    lower_bound = q1 - (1.5 * iqr)\n",
    "    range = [lower_bound, upper_bound]\n",
    "    print(f\"range in {col}:\",range)\n",
    "    \n",
    "    # checking the maximum value \n",
    "    max_value = df[col].max()\n",
    "    print(f\"The maximum value in {col} is: {max_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0b1a1",
   "metadata": {},
   "source": [
    " - The outliers are valid data points.For example, in the feature n_tokens_content, it shows  Number of words in the content the maximum value is 79.43, while the upper bound is 45.47, therefore it is a valid data value.\n",
    "\n",
    "- Similarly, num_hrefs is the Number of links,comparing the upper bound which is  4 with the maximum value 7  also indicates a valid data point.\n",
    "\n",
    "- Looking  for num_imgs which represents Number of images,comparing the upper bound with the maximum value also indicates a valid data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259d3191-8ee2-4ec8-9792-232bd441dffb",
   "metadata": {},
   "source": [
    "#### Null error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc05cbb-c460-41de-a8d0-b42034ddaa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "class_counts = df['shares_level'].value_counts()\n",
    "print(\"Class distribution:\\n\", class_counts)\n",
    "\n",
    "# Calculate class distribution percentages\n",
    "class_percentage = df['shares_level'].value_counts(normalize=True) * 100\n",
    "print(\"\\nClass distribution percentages:\\n\", class_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7146b4-e7df-4ee3-9d52-1cb025e4e952",
   "metadata": {},
   "source": [
    "- Our Target column is not imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8399ea",
   "metadata": {},
   "source": [
    "<b> [Back to Content](#Content) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c87f9",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab40ef6b-3917-41a6-8483-31db239c9e3d",
   "metadata": {},
   "source": [
    "## 5. Prepped Data Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce46e6-830f-44b5-a784-ab71b10df49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking every columns has the correct data types.\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dbb111-c97f-42d7-b194-1d922938474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that there are no duplicates.\n",
    "\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94def905-f3a4-4b15-8300-8d78c2d8bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the descriptive statistics.\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a9ea3-50c9-4ef6-b2fd-097581e26df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring that there is no null value present.\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d020a6d8-7edb-450c-802d-15bf6bab4c6f",
   "metadata": {},
   "source": [
    "#### Ensuring Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b4c6d-2cf4-4ba3-a451-5a86899b3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map categories to colors\n",
    "color_mapping = { 'low': '#FFD700', 'medium': '#FF5733','high': '#7CFC00'} \n",
    "\n",
    "sns.countplot(data = df, x = 'shares_level', palette=color_mapping.values())\n",
    "\n",
    "# Summary statistics for 'reg_pct_level'\n",
    "display(df['shares_level'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c15af4d",
   "metadata": {},
   "source": [
    "- The new feature shares_level  has three unique values, with medium having the highest count of 22,887, followed by high and low in second and third place, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ac038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the function\n",
    "box_dist_plot(df,'n_tokens_title')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69067048",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4660324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'n_tokens_content')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3b90b",
   "metadata": {},
   "source": [
    "- There are  visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45594c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'n_unique_tokens')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdbd6bf",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb99d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'n_non_stop_words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbbd417",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dba2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'n_non_stop_unique_tokens')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6547fd62",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c68b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'num_hrefs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4f487",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddac837",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'num_self_hrefs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b25a6",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4c3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'num_imgs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b8002c",
   "metadata": {},
   "source": [
    "- There are no  visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87370e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'num_videos')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b13ad7",
   "metadata": {},
   "source": [
    "- There are no  visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374daec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'average_token_length')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f1f1c",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution changed to normal to right-skewed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5536e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'num_keywords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604a16f",
   "metadata": {},
   "source": [
    "- There is only one  visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e5e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'kw_min_min')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b662b4",
   "metadata": {},
   "source": [
    "- There are  no visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b34511",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'kw_max_min')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f48d2",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5887cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'kw_avg_min')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aba7030",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e15425",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'kw_min_max')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85080326",
   "metadata": {},
   "source": [
    "- There are no  visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad85128",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'kw_max_max')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69412c4",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d5474",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'kw_avg_max')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43750f5",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee3108",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'kw_min_avg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31eee7",
   "metadata": {},
   "source": [
    "- There are no visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c02fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'kw_max_avg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241c0a24",
   "metadata": {},
   "source": [
    "-  There are visible outliers, and the distribution  changed to normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01a8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'kw_avg_avg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e035c2",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution changed to normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136392d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'self_reference_min_shares')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b550b0",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b535171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'self_reference_max_shares')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec11de7",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92acb72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'self_reference_avg_sharess')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1a462",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c779085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'LDA_00')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef99362f",
   "metadata": {},
   "source": [
    "- There are  no visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a5bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'LDA_01')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5450a35a",
   "metadata": {},
   "source": [
    "- There are no  visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ea728",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'LDA_02')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9611355",
   "metadata": {},
   "source": [
    "- There are  no visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d7f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'LDA_03')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d796056",
   "metadata": {},
   "source": [
    "- There are no  visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'LDA_04')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09463413",
   "metadata": {},
   "outputs": [],
   "source": [
    "- There are visible no  outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d9e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'global_subjectivity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c129efd",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f15d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'global_sentiment_polarity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd1bd8",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9964216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'global_rate_positive_words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7824bc2f",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac66e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'global_rate_negative_words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8537934",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85593494",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'rate_positive_words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f666f7e9",
   "metadata": {},
   "source": [
    "- There are no  visible outliers, and the distribution changed to left skewed to normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329133a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'rate_negative_words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153aa978",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8890289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'avg_positive_polarity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5498ff",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ba8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'min_positive_polarity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff050848",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f65f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'max_positive_polarity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7357dc",
   "metadata": {},
   "source": [
    "- There are no  visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d281aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'avg_negative_polarity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbad050d",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'min_negative_polarity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ec62b9",
   "metadata": {},
   "source": [
    "- There are no  visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0daea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'max_negative_polarity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2764a9",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'title_subjectivity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c97cb",
   "metadata": {},
   "source": [
    "- There are  no visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ec4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'title_sentiment_polarity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fff952",
   "metadata": {},
   "source": [
    "- There are visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'abs_title_subjectivity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4cf43f",
   "metadata": {},
   "source": [
    "- There are no visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4815d255",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_dist_plot(df, 'abs_title_sentiment_polarity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca94a86",
   "metadata": {},
   "source": [
    "- There are no visible outliers, and the distribution shows no significant change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151b82fb-585d-4f60-a5e8-ab056614bc99",
   "metadata": {},
   "source": [
    "#### Ensuring Bivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae47ae7e-bfab-4826-94a2-258ecd3abcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatterplot  to find the insight depends on n_unique_tokens and n_non_stop_words Column.\n",
    "sns.scatterplot(x='n_unique_tokens', y='n_non_stop_words' ,data=df, color='c') \n",
    "plt.title('Scatter Plot:Relation between n_unique_tokens and n_non_stop_words ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f8f1bd-ca55-4156-8087-81eddaeebc4d",
   "metadata": {},
   "source": [
    "- It has strong Positive correlation of .76.\n",
    "- When the number of n_unique_tokens increases then n_non_stop_words will aslo increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a50876-6e75-4d58-9b3b-f0cd92cd9555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatterplot  to find the insight depends on n_unique_tokens and n_non_stop_unique_tokens Column.\n",
    "\n",
    "sns.scatterplot(x='n_unique_tokens', y='n_non_stop_unique_tokens' ,data=df, color='c')  \n",
    "plt.title('Scatter Plot:Relation between n_unique_tokens and n_non_stop_unique_tokens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b9a808-3f48-486d-adaf-c0009b24c7ab",
   "metadata": {},
   "source": [
    "- It has strong Positive correlation of .95.\n",
    "- When the number of n_unique_tokens increases then n_non_stop_unique_tokens will aslo increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777b43f-5770-4a91-83e4-e472063b79cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatterplot  to find the insight depends on n_non_stop_words and n_non_stop_unique_tokens Column.\n",
    "\n",
    "sns.scatterplot(x='n_non_stop_words', y='n_non_stop_unique_tokens' ,data=df, color='c')  \n",
    "plt.title('Scatter Plot:Relation between n_non_stop_words and n_non_stop_unique_tokens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d4349d-0f99-4ddc-91d4-673fe8d0b5f8",
   "metadata": {},
   "source": [
    "- It has strong Positive correlation of .84.\n",
    "- When the number of n_non_stop_words increases then n_non_stop_unique_tokens will aslo increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931ace8b-ddcd-4064-b134-5ec941445eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatterplot  to find the insight depends on avg_negative_polarity and min_negative_polarity Column.\n",
    "\n",
    "sns.scatterplot(x='avg_negative_polarity', y='min_negative_polarity' ,data=df, color='c')  \n",
    "plt.title('Scatter Plot:Relation between avg_negative_polarity and min_negative_polarity ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3193f6ce-fb53-4d56-a812-54be60d3bab5",
   "metadata": {},
   "source": [
    "- It has positive correlation of .77\n",
    "- When the number of avg_negative_polarity increases then min_negative_polarity will aslo increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3677a0f4-3da8-4e49-8259-63443631f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatterplot  to find the insight depends on abs_title_sentiment_polarity and title_sentiment_polarity Column.\n",
    "\n",
    "sns.scatterplot(x='abs_title_sentiment_polarity', y='title_sentiment_polarity' ,data=df, color='c')  # code to plot bargraph\n",
    "plt.title('Scatter Plot:Relation between abs_title_sentiment_polarity and title_sentiment_polarity ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa701a5-3bd8-41bb-b40d-36f4cd5d7ce4",
   "metadata": {},
   "source": [
    "- There is no clear linear relation between the two columns.\n",
    "- They are mirror image . So we can not tell, which is increasing and which is decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567dd1e1-a488-482f-8cd1-20c49c65e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatterplot  to find the insight depends on title_subjectivity and abs_title_sentiment_polarity Column.\n",
    "\n",
    "sns.scatterplot(x='title_subjectivity', y='abs_title_sentiment_polarity' ,data=df, color='c')  # code to plot bargraph\n",
    "plt.title('Scatter Plot:Relation between title_subjectivity and abs_title_sentiment_polarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5ceea-cadd-4746-871f-335c8bf71263",
   "metadata": {},
   "source": [
    "- It has positive correlation of .83\n",
    "- When the number of title_subjectivity increases then abs_title_sentiment_polarity will aslo increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90b887-0d6f-4d71-8a4d-8194a1e4a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatterplot  to find the insight depends on title_subjectivity and abs_title_subjectivity Column.\n",
    "\n",
    "sns.scatterplot(x='title_subjectivity', y='abs_title_subjectivity' ,data=df, color='c')  # code to plot bargraph\n",
    "plt.title('Scatter Plot:Relation between title_subjectivity and abs_title_subjectivity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ff9c2-2e72-46fa-9502-cb1e0e5b474f",
   "metadata": {},
   "source": [
    "- It has negative correlation of .70\n",
    "- When the number of title_subjectivity decrease then abs_title_subjectivity will aslo decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c4e1f-4e16-45d0-8875-1816b7b624e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatterplot  to find the insight depends on title_sentiment_polarity and abs_title_sentiment_polarity Column.\n",
    "\n",
    "sns.scatterplot(x='title_sentiment_polarity', y='abs_title_sentiment_polarity' ,data=df, color='c')  # code to plot bargraph\n",
    "plt.title('Scatter Plot: Relation between title_sentiment_polarity and abs_title_sentiment_polarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9e98e9-ec30-4dbf-928d-1e2c460bae83",
   "metadata": {},
   "source": [
    "- There is no clear linear relation between the two columns.\n",
    "- They are mirror image . So we can not tell, which is increasing and which is decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b128021-9c37-45db-8054-786a82ec1edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_categorical_vs_categorical(df, cat_feature1, cat_feature2):\n",
    "    \"\"\"\n",
    "    Plots a bar plot to visualize the relationship between two categorical features.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The dataframe containing the data.\n",
    "    cat_feature1 (str): The first categorical feature (x-axis).\n",
    "    cat_feature2 (str): The second categorical feature (used for hue).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(data=df, x=cat_feature1, hue=cat_feature2)\n",
    "    plt.title(f'Bar Plot of {cat_feature1} vs {cat_feature2}')\n",
    "    plt.xlabel(cat_feature1)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend(title=cat_feature2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ff186-3618-462c-9e8a-97b4ab19d6e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_bar_categorical_vs_categorical(df, 'day_of_week', 'shares_level')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e6aa7",
   "metadata": {},
   "source": [
    "- In each day of the week the level of the shares is medium which has highest count.\n",
    "- In each day of the week the level of the shares is high which has lowest count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e4b956",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_bar_categorical_vs_categorical(df, 'data_channels', 'shares_level')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed61c7",
   "metadata": {},
   "source": [
    "- The count of share level of the 'world' data channels is maximum in medium share level range.\n",
    "- The count of share level of the 'Tech' data channels is maximum in high share level range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3d0bb4-50ed-4671-8328-687704426127",
   "metadata": {},
   "source": [
    "#### Ensuring Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7da7e38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate mean values of each numerical feature grouped by target class\n",
    "multivariate_summary = df.groupby('shares_level')[['global_rate_positive_words',  'global_rate_negative_words']].mean()\n",
    "\n",
    "# Display the result\n",
    "print(\"Multivariate Analysis (Mean values by shares_level):\")\n",
    "print(multivariate_summary)\n",
    "\n",
    "# Plotting bar chart for mean values by shares_level\n",
    "multivariate_summary.plot(kind='bar', figsize=(12, 6), color=['lightgreen', 'orange'], width=0.8)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Mean Values of global_rate_positive_words and global_rate_negative_words by shares_level')\n",
    "plt.xlabel('shares_level')\n",
    "plt.ylabel('Mean Value')\n",
    "plt.xticks(rotation=45, ha='right')  \n",
    "plt.tight_layout()\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d06741",
   "metadata": {},
   "source": [
    "- The mean value of 'global_rate_positive_words' is maximum when the share level is high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36110b2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate mean values of each numerical feature grouped by target class\n",
    "multivariate_summary = df.groupby('shares_level')[['rate_positive_words',  'rate_negative_words']].mean()\n",
    "\n",
    "# Display the result\n",
    "print(\"Multivariate Analysis (Mean values by shares_level):\")\n",
    "print(multivariate_summary)\n",
    "\n",
    "# Plotting bar chart for mean values by shares_level\n",
    "multivariate_summary.plot(kind='bar', figsize=(12, 6), color=['lightgreen', 'orange'], width=0.8)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Mean Values of rate_positive_words and rate_negative_words by shares_level')\n",
    "plt.xlabel('shares_level')\n",
    "plt.ylabel('Mean Value')\n",
    "plt.xticks(rotation=45, ha='right') \n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509600cd",
   "metadata": {},
   "source": [
    "- The mean value of 'rate_positive_words' is maximum when the share level is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a6441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean values of each numerical feature grouped by target class\n",
    "multivariate_summary = df.groupby('data_channels')[['rate_positive_words',  'rate_negative_words']].mean()\n",
    "\n",
    "# Display the result\n",
    "print(\"Multivariate Analysis (Mean values by data_channels):\")\n",
    "print(multivariate_summary)\n",
    "\n",
    "# Plotting bar chart for mean values by data_channels\n",
    "multivariate_summary.plot(kind='bar', figsize=(12, 6), color=['lightgreen', 'orange'], width=0.8)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Mean Values of rate_positive_words and rate_negative_words by data_channels')\n",
    "plt.xlabel('data_channels')\n",
    "plt.ylabel('Mean Value')\n",
    "plt.xticks(rotation=45, ha='right')  \n",
    "plt.tight_layout()\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ad5f9",
   "metadata": {},
   "source": [
    "- The mean value of the count of the rate of positive word is maximum for data channel \"Tech\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a7738c",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<b> [Back to Content](#Content) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be78bb",
   "metadata": {},
   "source": [
    "## Feature Selection / Dimensionality Reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd7cea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31073ecc-4fe6-43aa-849c-4e9d813d6da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting y to number coding as 'low'=1, 'medium'=2, 'high'=3.\n",
    "\n",
    "df['shares_level'] = df['shares_level'].map({'low': 1, 'medium': 2, 'high': 3})\n",
    "\n",
    "# Seperating target attribute from rest of the attributes.\n",
    "y = df['shares_level']\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb6cea-5d32-49e5-8b77-dbb2e5a4faf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the 'shares_level' column\n",
    "\n",
    "X = df.drop('shares_level', axis=1)\n",
    "\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c3d06-163e-48c1-9e6c-6de0dd96c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dummy data from the categorical columns.\n",
    "\n",
    "cat_cols = ['data_channels']\n",
    "\n",
    "X_cat_dummy = pd.get_dummies(X[cat_cols], drop_first=True).astype(int)\n",
    "\n",
    "X_cat_dummy.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f868e2-dbc0-4a9b-89fd-f9db05ce9557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original columns from the DataFrame.\n",
    "\n",
    "X.drop(columns=cat_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d94333-c910-417f-a697-90b72d117802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarise the numeric attributes.\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "X_std = std_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf8256",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21648b18-321c-4af4-91e1-2afd19a5b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the nd array of X_std to dataframe.\n",
    "\n",
    "a = X.columns\n",
    "X_std = pd.DataFrame(X_std, columns = a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e89344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Index to merge correctly.\n",
    "\n",
    "X_std.reset_index(inplace=True, drop=True)\n",
    "X_cat_dummy.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f56aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the original DataFrame with the one-hot encoded columns.\n",
    "\n",
    "X = pd.concat([X_std, X_cat_dummy], axis=1)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of dataframe having independent attributes.\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3678dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train & test split.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd0885",
   "metadata": {},
   "source": [
    "- Training and testing data is split 70-30 of X,y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d772e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of the resulting datasets.\n",
    "\n",
    "print(\"Training dataset shapes ->  X: {}, y: {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"Testing dataset shapes  ->  X: {}, y: {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43ccf3",
   "metadata": {},
   "source": [
    "### Variance Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10697f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the VarianceThreshold object (remove features with variance below the threshold)\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "\n",
    "# Fit the selector to the data\n",
    "selector.fit(X_train)\n",
    "\n",
    "selector.get_support()\n",
    "\n",
    "# Get the list of featurs with low variance.\n",
    "low_var_cols = [col for col in X.columns if col not in X.columns[selector.get_support()]]\n",
    "\n",
    "print(f\"Total number of attributes with Low Variance : {len(low_var_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4faae26",
   "metadata": {},
   "source": [
    "- There is no any attributes with Low Variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678aeb5e",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<b> [Back to Content](#Content) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db151d80",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afaa2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributing X_train & X_test to numerical & categorical attributes respectively.\n",
    "\n",
    "X_train_num = X_train.select_dtypes(include='float64')\n",
    "\n",
    "X_train_cat = X_train.select_dtypes(exclude='float64')\n",
    "\n",
    "\n",
    "X_test_num = X_test.select_dtypes(include='float64')\n",
    "\n",
    "X_test_cat = X_test.select_dtypes(exclude ='float64') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e3e4d",
   "metadata": {},
   "source": [
    "- Since Principal Component Analysis (PCA) is designed for numerical features, we have removed the categorical columns from our dataset.In order to enhance our ability to investigate and show the underlying patterns and correlations in the data, we make sure that our PCA transformation only concentrates on the numerical characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8276322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a PCA model, and set the components to \n",
    "pca = PCA(n_components=59)\n",
    "\n",
    "# apply the pca and transform it to 16 principal components per observation of the standarised data 'X_train_num'.\n",
    "X_train_num_pca = pca.fit(X_train_num)\n",
    "\n",
    "# Display the explained variance ratio for the principal components we've derived from the data 'X_train_num'.\n",
    "print(np.round(pca.explained_variance_ratio_, 2)*100)\n",
    "\n",
    "# apply the pca and transform it to 16 principal components per observation of the standarised data 'X_test_num'.\n",
    "X_test_num_pca = pca.fit(X_test_num)\n",
    "\n",
    "# Display the explained variance ratio for the principal components we've derived from the data 'X_test_num'.\n",
    "print(np.round(pca.explained_variance_ratio_, 2)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21849efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance ratio in percentage for all 16 numeric features\n",
    "evp = np.array([11, 8, 7, 5, 5, 5, 4, 4, 4, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, \n",
    "                2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, \n",
    "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "epr = evp / 100\n",
    "\n",
    "cev = np.cumsum(epr)\n",
    "\n",
    "# Find the number of components that explain at least 90% of the variance.\n",
    "\n",
    "n_compt = np.argmax(cev >= 0.90) + 1\n",
    "print(f\"No. of components that explain at least 90% of the variance: {n_compt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2d54a",
   "metadata": {},
   "source": [
    "- So, we will be selecting 28 No. of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c628c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of a PCA model, and set the components to 28, from the above results.\n",
    "pca = PCA(n_components=28)\n",
    "\n",
    "# Apply the results of the PCA to training & testing data to transform it into 5 principal components\n",
    "\n",
    "X_train_num_pca = pca.fit_transform(X_train_num)\n",
    "X_test_num_pca = pca.fit_transform(X_test_num)\n",
    "\n",
    "# Convert the nd array of X_train_num_pca, & X_test_num to dataframe.\n",
    "X_train_num_pca = pd.DataFrame(X_train_num_pca, columns=['pca_1', 'pca_2', 'pca_3', 'pca_4', 'pca_5', 'pca_6', \n",
    "                                                         'pca_7', 'pca_8', 'pca_9', 'pca_10', 'pca_11', 'pca_12', \n",
    "                                                         'pca_13', 'pca_14', 'pca_15', 'pca_16', 'pca_17', 'pca_18', \n",
    "                                                         'pca_19', 'pca_20', 'pca_21', 'pca_22', 'pca_23', 'pca_24', \n",
    "                                                         'pca_25', 'pca_26', 'pca_27', 'pca_28'])\n",
    "print('X_train_num_pca :- ')\n",
    "display(X_train_num_pca.head(2))\n",
    "print(X_train_num_pca.shape)\n",
    "\n",
    "print()\n",
    "\n",
    "X_test_num_pca = pd.DataFrame(X_test_num_pca, columns=['pca_1', 'pca_2', 'pca_3', 'pca_4', 'pca_5', 'pca_6', \n",
    "                                                         'pca_7', 'pca_8', 'pca_9', 'pca_10', 'pca_11', 'pca_12', \n",
    "                                                         'pca_13', 'pca_14', 'pca_15', 'pca_16', 'pca_17', 'pca_18', \n",
    "                                                         'pca_19', 'pca_20', 'pca_21', 'pca_22', 'pca_23', 'pca_24', \n",
    "                                                         'pca_25', 'pca_26', 'pca_27', 'pca_28'])\n",
    "print('X_test_num_pca :- ')\n",
    "display(X_test_num_pca.head(2))\n",
    "print(X_test_num_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa2338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index so that it does not provide null values while concatination.\n",
    "\n",
    "X_train_num_pca.reset_index(drop=True, inplace=True)\n",
    "X_train_cat.reset_index(drop=True, inplace=True)\n",
    "X_test_num_pca.reset_index(drop=True, inplace=True)\n",
    "X_test_cat.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb562478",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5128dad2",
   "metadata": {},
   "source": [
    "- Thus, we have used the PCA and eliminated attributes from the numerical attributes, and finally we have only 28 PCAs' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0de21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concating the numerical & categorical attributes before model training.\n",
    "\n",
    "X_train = pd.concat([X_train_num_pca , X_train_cat], axis=1)\n",
    "\n",
    "X_test = pd.concat([X_test_num_pca , X_test_cat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3547d96",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<b> [Back to Content](#Content) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9033a8",
   "metadata": {},
   "source": [
    "### Forward Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1dba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the linear regression model\n",
    "\n",
    "lreg = LinearRegression()\n",
    "sfs1 = sfs(lreg, k_features=20, forward=True, verbose=2, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Sequential Feature Selector model_1\n",
    "sfs1 = sfs1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e5772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the names of the selected features after the fitting process.\n",
    "\n",
    "feat_names_1 = list(sfs1.k_feature_names_)\n",
    "feat_names_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade0e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the selected feature names for separation\n",
    "selected_features = feat_names_1\n",
    "\n",
    "# Creating a new DataFrame with the selected features\n",
    "X_train_best = X_train[selected_features]\n",
    "X_test_best = X_test[selected_features]\n",
    "\n",
    "# Display the shape of the new DataFrame to confirm the selection\n",
    "\n",
    "display(X_train_best.shape)\n",
    "display(X_train_best.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c99282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shapes of the final resulting datasets before training the models.\n",
    "\n",
    "print(\"All best features Training dataset shapes ->  X: {}, y: {}\".format(X_train_best.shape, y_train.shape))\n",
    "print(\"All best features Testing dataset shapes  ->  X: {}, y: {}\".format(X_test_best.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c40b4",
   "metadata": {},
   "source": [
    "- Here we performed forward selection and identified 20 best features, which will be used to train the models.\n",
    "  \n",
    "- We will keep the 20 best feature for all the models to similar dataset so to check the performance of different models, and hence we can conclude that which models is performing the best.\n",
    "\n",
    "- Feature selection can highlight the most significant variables influencing the outcome, helping analyst to gain insights into which features are most relevant for the problem.\n",
    "\n",
    "- By selecting only the most important features, feature selection helps reduce unnecessary information, making the model's predictions more accurate and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb52487",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<b> [Back to Content](#Content) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802cf25e",
   "metadata": {},
   "source": [
    "## Neural Network Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f4ceb",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286fbc81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Smaller neural network model \n",
    "# First hidden layer with fewer neurons\n",
    "model_1 = Sequential([\n",
    "    Dense(32, input_dim=X_train_best.shape[1], activation='relu'),  \n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "model_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_1.fit(X_train_best, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob_1 = model_1.predict(X_test_best)\n",
    "y_pred_1 = (y_pred_prob_1 > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_1 = accuracy_score(y_test, y_pred_1)\n",
    "precision_1 = precision_score(y_test, y_pred_1, average='macro')\n",
    "recall_1 = recall_score(y_test, y_pred_1, average='macro')\n",
    "f1_1 = f1_score(y_test, y_pred_1, average='macro')\n",
    "\n",
    "# Print metrics for model 1\n",
    "print(f\"Model 1 - Accuracy: {accuracy_1}\")\n",
    "print(f\"Model 1 - Precision: {precision_1}\")\n",
    "print(f\"Model 1 - Recall: {recall_1}\")\n",
    "print(f\"Model 1 - F1 Score: {f1_1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fed5fb3",
   "metadata": {},
   "source": [
    "A basic model with two layers (64 and 32 neurons). It's used as a starting point to check if a simple neural network can perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42da158",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<b> [Back to Content](#Content) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55a9a00",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa1f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Deeper neural network model\n",
    "model_2 = Sequential([\n",
    "    Dense(128, input_dim=X_train_best.shape[1], activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "model_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_2.fit(X_train_best, y_train, epochs=30, batch_size=64, verbose=0)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob_2 = model_2.predict(X_test_best)\n",
    "y_pred_2 = (y_pred_prob_2 > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_2 = accuracy_score(y_test, y_pred_2)\n",
    "precision_2 = precision_score(y_test, y_pred_2, average='macro')\n",
    "recall_2 = recall_score(y_test, y_pred_2, average='macro')\n",
    "f1_2 = f1_score(y_test, y_pred_2, average='macro')\n",
    "\n",
    "# Print metrics for model 2\n",
    "print(f\"Model 2 - Accuracy: {accuracy_2}\")\n",
    "print(f\"Model 2 - Precision: {precision_2}\")\n",
    "print(f\"Model 2 - Recall: {recall_2}\")\n",
    "print(f\"Model 2 - F1 Score: {f1_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b416e32",
   "metadata": {},
   "source": [
    "-  A model with more layers and neurons (128, 64, and 32 neurons). This model is designed to see if adding more complexity improves performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf8b26d",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<b> [Back to Content](#Content) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1e8eb9",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Neural network model with dropout for regularization\n",
    "model_3 = Sequential([\n",
    "    Dense(64, input_dim=X_train_best.shape[1], activation='relu'),\n",
    "    Dropout(0.3),  \n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),  \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "model_3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_3.fit(X_train_best, y_train, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob_3 = model_3.predict(X_test_best)\n",
    "y_pred_3 = (y_pred_prob_3 > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_3 = accuracy_score(y_test, y_pred_3)\n",
    "precision_3 = precision_score(y_test, y_pred_3, average='macro')\n",
    "recall_3 = recall_score(y_test, y_pred_3, average='macro')\n",
    "f1_3 = f1_score(y_test, y_pred_3, average='macro')\n",
    "\n",
    "# Print metrics for model 3\n",
    "print(f\"Model 3 - Accuracy: {accuracy_3}\")\n",
    "print(f\"Model 3 - Precision: {precision_3}\")\n",
    "print(f\"Model 3 - Recall: {recall_3}\")\n",
    "print(f\"Model 3 - F1 Score: {f1_3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408def67",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<b> [Back to Content](#Content) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb48075",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2101296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Index to merge correctly.\n",
    "\n",
    "X_train_best.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da87661",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Define a function to create a simple neural network model\n",
    "def create_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, input_dim=input_dim, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification output\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and train Model 1\n",
    "model_1 = create_model(X_train_best.shape[1])\n",
    "history_1 = model_1.fit(X_train_best, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "# Predict on the test set for Model 1\n",
    "y_pred_prob_1 = model_1.predict(X_test_best)  \n",
    "y_pred_1 = (y_pred_prob_1 > 0.5).astype(int)  \n",
    "\n",
    "# Calculate metrics for Model 1\n",
    "accuracy_1 = accuracy_score(y_test, y_pred_1)\n",
    "precision_1 = precision_score(y_test, y_pred_1, average='macro')\n",
    "recall_1 = recall_score(y_test, y_pred_1, average='macro')\n",
    "f1_1 = f1_score(y_test, y_pred_1, average='macro')\n",
    "\n",
    "# Print Model 1 metrics\n",
    "print(f\"Model 1 - Accuracy: {accuracy_1}\")\n",
    "print(f\"Model 1 - Precision: {precision_1}\")\n",
    "print(f\"Model 1 - Recall: {recall_1}\")\n",
    "print(f\"Model 1 - F1 Score: {f1_1}\")\n",
    "\n",
    "# Create and train Model 2 (with different architecture)\n",
    "def create_model_2(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=input_dim, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_2 = create_model_2(X_train_best.shape[1])\n",
    "history_2 = model_2.fit(X_train_best, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "# Predict on the test set for Model 2\n",
    "y_pred_prob_2 = model_2.predict(X_test_best)\n",
    "y_pred_2 = (y_pred_prob_2 > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics for Model 2\n",
    "accuracy_2 = accuracy_score(y_test, y_pred_2)\n",
    "precision_2 = precision_score(y_test, y_pred_2, average='macro')\n",
    "recall_2 = recall_score(y_test, y_pred_2, average='macro')\n",
    "f1_2 = f1_score(y_test, y_pred_2, average='macro')\n",
    "\n",
    "# Print Model 2 metrics\n",
    "print(f\"Model 2 - Accuracy: {accuracy_2}\")\n",
    "print(f\"Model 2 - Precision: {precision_2}\")\n",
    "print(f\"Model 2 - Recall: {recall_2}\")\n",
    "print(f\"Model 2 - F1 Score: {f1_2}\")\n",
    "\n",
    "# Create and train Model 3 \n",
    "def create_model_3(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(256, input_dim=input_dim, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_3 = create_model_3(X_train_best.shape[1])\n",
    "history_3 = model_3.fit(X_train_best, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "# Predict on the test set for Model 3\n",
    "y_pred_prob_3 = model_3.predict(X_test_best)\n",
    "y_pred_3 = (y_pred_prob_3 > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics for Model 3\n",
    "accuracy_3 = accuracy_score(y_test, y_pred_3)\n",
    "precision_3 = precision_score(y_test, y_pred_3, average='macro')\n",
    "recall_3 = recall_score(y_test, y_pred_3, average='macro')\n",
    "f1_3 = f1_score(y_test, y_pred_3, average='macro')\n",
    "\n",
    "# Print Model 3 metrics\n",
    "print(f\"Model 3 - Accuracy: {accuracy_3}\")\n",
    "print(f\"Model 3 - Precision: {precision_3}\")\n",
    "print(f\"Model 3 - Recall: {recall_3}\")\n",
    "print(f\"Model 3 - F1 Score: {f1_3}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24fa34",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<b> [Back to Content](#Content) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56244665",
   "metadata": {},
   "source": [
    "---------\n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479904ee",
   "metadata": {},
   "source": [
    "- Wednesday, followed by tuesday, and then thursday has the max counts of shares\n",
    "- Data Channel 'World' has the maximum count of shares on Tuesday , Wednesday , Thrusday , Friday and Sunday.\n",
    "- On Monday Entertainment channel has the maximum count of shares.\n",
    "- On Saturday Tech channel has the maximum count of shares.\n",
    "- Model 1 (Simple Architecture): We used fewer neurons (32) and fewer epochs (20) to keep the model simple and avoid heavy computation. This serves as a baseline to see if a basic model can find patterns without overfitting.\n",
    "\n",
    "- Model 2 (Deeper Neural Network): In this model, we added more layers (128 and 64 neurons) and increased the epochs (30) to help the model learn more complex patterns. A batch size of 64 was chosen to balance training speed and stability.\n",
    "\n",
    "- Model 3 (Regularized Neural Network): We added dropout layers (30% rate) to prevent overfitting by randomly turning off some neurons during training. A smaller batch size (16) and more epochs (50) were used to give the model more chances to learn effectively and improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d7d63",
   "metadata": {},
   "source": [
    "<b> [Back to top](#Neural-Networks) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57218e8d",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
